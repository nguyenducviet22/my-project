<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>How to enable OpenTelemetry traces in React applications</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/22/how-enable-opentelemetry-traces-react-applications" /><author><name>Purva Naik</name></author><id>c6739437-ace4-4f9f-af8c-3ae22cbf7684</id><updated>2023-03-22T07:00:00Z</updated><published>2023-03-22T07:00:00Z</published><summary type="html">&lt;p&gt;The main focus of this article is to demonstrate how to instrument React applications to make them observable. For a good overview of observability and OpenTelemetry, please take a look at the article, &lt;a href="https://developers.redhat.com/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help#why_is_observability_important_"&gt;Observability in 2022: Why it matters and how OpenTelemetry can help&lt;/a&gt;.&lt;/p&gt; &lt;ul&gt;&lt;/ul&gt;&lt;h2&gt;10-step OpenTelemetry demonstration&lt;/h2&gt; &lt;p&gt;Related to the OpenTelemetry, we are using the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Auto instrumentation via &lt;a href="https://github.com/open-telemetry/opentelemetry-js/tree/main/packages/opentelemetry-sdk-trace-web"&gt;sdk-trace-web&lt;/a&gt; and a plugin to provide auto instrumentation for &lt;a href="https://github.com/open-telemetry/opentelemetry-js/tree/main/experimental/packages/opentelemetry-instrumentation-fetch#opentelemetry-fetch-instrumentation-for-web"&gt;fetch&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;OpenTelemetry Collector (also known as &lt;code&gt;OTELCOL&lt;/code&gt;).&lt;/li&gt; &lt;li&gt;Jaeger&lt;/li&gt; &lt;li&gt;Basic collector deployment pattern. For more information about &lt;code&gt;OTELCOL&lt;/code&gt; deployment patterns, please take a look at &lt;a href="https://github.com/jpkrohling/opentelemetry-collector-deployment-patterns"&gt;OpenTelemetry Collector Deployment Patterns&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Step 1. Set up prerequisites&lt;/h3&gt; &lt;p&gt;In this demo, we are going to use Docker and docker-compose. You can refer to &lt;a href="https://docs.docker.com/get-started/"&gt;Docker&lt;/a&gt; and &lt;a href="https://docs.docker.com/compose/"&gt;docker-compose&lt;/a&gt; to learn more.&lt;/p&gt; &lt;h3&gt;Step 2. Run the React application example&lt;/h3&gt; &lt;p&gt;You will be using a &lt;a href="https://github.com/obs-nebula/frontend-react"&gt;front-end react application&lt;/a&gt; that contains the sample React application code that we will instrument. Please note that the repository also contains an Express application as a back end, but the focus of this tutorial is to instrument the front end only.&lt;/p&gt; &lt;p&gt;The front-end application contains a button that calls the back end using Express and a scroll component that calls the &lt;a href="https://randomuser.me/"&gt;https://randomuser.me/&lt;/a&gt; free public API. We are going to delegate to OpenTelemetry libraries the job of capturing traces for the button and the scroll component. So every time the user clicks on the button or scrolls the page, the auto-instrumentation plugin will generate traces for this.&lt;/p&gt; &lt;p&gt;Clone the following GitHub repository from the command line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;git clone https://github.com/obs-nebula/frontend-react.git&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 3. Instrument the React application&lt;/h3&gt; &lt;p&gt;The following list shows the dependencies we added. You may want to use newer versions, depending on when you are reading this article:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-json"&gt;"@opentelemetry/exporter-trace-otlp-http": "^0.35.0", "@opentelemetry/instrumentation": "^0.35.0", "@opentelemetry/instrumentation-fetch": "^0.35.0", "@opentelemetry/resources": "^1.9.1", "@opentelemetry/sdk-trace-web": "^1.8.0", "@opentelemetry/semantic-conventions": "^1.9.1"&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Create a file named &lt;code&gt;tracing.js&lt;/code&gt; that will load OpenTelemetry. We are going to share more details in the following subsections. The content of the &lt;code&gt;front-end/src/tracing.js&lt;/code&gt; file is as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const { Resource } = require('@opentelemetry/resources'); const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions'); const { WebTracerProvider, SimpleSpanProcessor, ConsoleSpanExporter } = require('@opentelemetry/sdk-trace-web'); const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http'); const { registerInstrumentations } = require('@opentelemetry/instrumentation'); const { FetchInstrumentation } = require('@opentelemetry/instrumentation-fetch'); const consoleExporter = new ConsoleSpanExporter(); const collectorExporter = new OTLPTraceExporter({ headers: {} }); const provider = new WebTracerProvider({ resource: new Resource({ [SemanticResourceAttributes.SERVICE_NAME]: process.env.REACT_APP_NAME }) }); const fetchInstrumentation = new FetchInstrumentation({}); fetchInstrumentation.setTracerProvider(provider); provider.addSpanProcessor(new SimpleSpanProcessor(consoleExporter)); provider.addSpanProcessor(new SimpleSpanProcessor(collectorExporter)); provider.register(); registerInstrumentations({ instrumentations: [ fetchInstrumentation ], tracerProvider: provider }); export default function TraceProvider ({ children }) { return ( &lt;&gt; {children} &lt;/&gt; ); }&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 4. Import the required modules&lt;/h3&gt; &lt;p&gt;Next, you will need to import the OpenTelemetry modules. As you can see, we can get the &lt;code&gt;ConsoleSpanExporter&lt;/code&gt; and &lt;code&gt;SimpleSpanProcessor&lt;/code&gt; from the @opentelemetry/sdk-trace-web so we don’t need to add an extra dependency for this.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const { Resource } = require('@opentelemetry/resources'); const { SemanticResourceAttributes } = require('@opentelemetry/semantic-conventions'); const { WebTracerProvider,SimpleSpanProcessor, ConsoleSpanExporter } = require('@opentelemetry/sdk-trace-web'); const { OTLPTraceExporter } = require('@opentelemetry/exporter-trace-otlp-http'); const { registerInstrumentations } = require('@opentelemetry/instrumentation'); const { FetchInstrumentation } = require('@opentelemetry/instrumentation-fetch');&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 5. Initialize the tracer&lt;/h3&gt; &lt;p&gt;Since we are using React with JavaScript, to initialize the OpenTelemetry tracer, you will need to create a new instance of the &lt;code&gt;TraceProvider&lt;/code&gt; and pass it as a property to your root React component. You can do this by adding the following code to your main application file, in our case, that is in index.js file.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;import TraceProvider from './tracing'; const root = ReactDOM.createRoot(document.getElementById('root')); root.render( &lt;TraceProvider&gt; &lt;App /&gt; &lt;/TraceProvider&gt; );&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 6. Create an OTELCOL exporter instance&lt;/h3&gt; &lt;p&gt;To export the traces to &lt;code&gt;OTELCOL&lt;/code&gt;, you will need to create an instance of &lt;code&gt;OTLPTraceExporter&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Note that we are adding a workaround to use &lt;a href="https://developer.mozilla.org/en-US/docs/Glossary/XMLHttpRequest"&gt;XHR&lt;/a&gt; instead of &lt;a href="https://developer.mozilla.org/en-US/docs/Web/API/Navigator/sendBeacon"&gt;sendBeacon&lt;/a&gt;, as described in this OpenTelemetry JS upstream &lt;a href="https://github.com/open-telemetry/opentelemetry-js/issues/3062#issuecomment-1189189494"&gt;issue&lt;/a&gt;. With that, we can fix the CORS problem when exporting.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-javascript"&gt;const collectorExporter = new OTLPTraceExporter({ headers: {} });&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 7. Create the otel-collector-config file&lt;/h3&gt; &lt;p&gt;Now let’s take a look at the yaml file content. To configure the &lt;code&gt;OTELCOL&lt;/code&gt;, you will need to create a new file called &lt;code&gt;otel-collector-config.yaml&lt;/code&gt; in your root directory. In this file, we are going to configure the receiver, processor, and exporters (using Jaeger and logging as exporters).&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;receivers: otlp: protocols: http: cors: allowed_origins: ["*"] allowed_headers: ["*"] exporters: logging: verbosity: Detailed jaeger: endpoint: jaeger-all-in-one:14250 tls: insecure: true processors: batch: service: telemetry: logs: level: "debug" pipelines: traces: receivers: [otlp] exporters: [logging, jaeger] processors: [batch]&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 8. Create a docker compose file&lt;/h3&gt; &lt;p&gt;Create a docker-compose file and define the services for &lt;code&gt;OTELCOL&lt;/code&gt;, Jaeger, and the application as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;version: "2" services: front-end: build: context:./front-end depends_on: - express-server ports: - "3000:3000" env_file: -./front-end/src/.env express-server: build: context:./express-server ports: - "5000:5000" collector: image: otel/opentelemetry-collector:latest command: ["--config=/otel-collector-config.yaml"] volumes: - './otel-collector-config.yaml:/otel-collector-config.yaml' ports: - "4318:4318" depends_on: - jaeger-all-in-one # Jaeger jaeger-all-in-one: hostname: jaeger-all-in-one image: jaegertracing/all-in-one:latest ports: - "16685" - "16686:16686" - "14268:14268" - "14250:14250"&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Step 9. Start the services&lt;/h3&gt; &lt;p&gt;Once you have created &lt;code&gt;OTELCOL&lt;/code&gt; config and Docker compose files, you can start the services by running the following command in your terminal:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$docker-compose up&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the services are started you can access the react application at &lt;a href="http://localhost:3000"&gt;http://localhost:3000&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;As mentioned previously, every time the users scroll the page, it will generate random data through an API call and the OpenTelemetry will generate traces based on the scrolling activity.&lt;/p&gt; &lt;h3&gt;Step 10. View the traces in Jaeger&lt;/h3&gt; &lt;p&gt;You can view the traces in the Jaeger UI by navigating to &lt;a href="http://localhost:16686"&gt;http://localhost:16686&lt;/a&gt;. The horizontal circles in the chart represent button clicks, and the vertical circles represent scrolling activity, as shown in Figure 1.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/jaeger.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/jaeger.png?itok=a8j9x5tC" width="600" height="290" alt="The jaeger UI showing circles in the chart that represent button clicks and scrolling activity." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Application tracing in Jaeger.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Let's click on one of the horizontal items and expand the trace detail as shown in Figure 2:&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/2023-02-24_09-14.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/2023-02-24_09-14.png?itok=XJ69RS8q" width="600" height="304" alt="In the Jaeger UI, expanded trace detail shows the Express back-end was called and the OpenTelemetry library name." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The expanded trace detail shows the Express back-end was called and the OpenTelemetry library name.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;We can see in Figure 2 that the Express back end was called, and the OpenTelemetry library name that is responsible for this trace generation.&lt;/p&gt; &lt;p&gt;Now let’s click in one vertical circle as shown in Figure 3:&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="align-center media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/type.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/type.png?itok=qb9l_okb" width="600" height="345" alt="The trace information showing that the scrolling activity is calling an external API." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: The trace information showing that the scrolling activity is calling an external API.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;We can see in Figure 3 that scrolling activity is calling an external API.&lt;/p&gt; &lt;h2&gt;Collecting and analyzing telemetry data&lt;/h2&gt; &lt;p&gt;You have successfully enabled OpenTelemetry in your React application using the OpenTelemetry collector and Jaeger. You can now start collecting and analyzing telemetry data. You can use the Jaeger UI to view traces, identify performance bottlenecks, and gain deeper understanding about what the React application is doing when calling external systems.&lt;/p&gt; &lt;h2&gt;Further reading&lt;/h2&gt; &lt;p&gt;Want to learn more about observability and OpenTelemetry? Check out these articles:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/04/12/observability-2022-why-it-matters-and-how-opentelemetry-can-help"&gt;Observability in 2022: Why it matters and how OpenTelemetry can help&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/articles/2022/06/21/distributed-tracing-opentelemetry-knative-and-quarkus"&gt;Distributed tracing with OpenTelemetry, Knative, and Quarkus&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/blog/2019/05/01/a-guide-to-the-open-source-distributed-tracing-landscape"&gt;A guide to the open source distributed tracing landscape&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/22/how-enable-opentelemetry-traces-react-applications" title="How to enable OpenTelemetry traces in React applications"&gt;How to enable OpenTelemetry traces in React applications&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Purva Naik</dc:creator><dc:date>2023-03-22T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.16.5.Final released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-16-5-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-16-5-final-released/</id><updated>2023-03-22T00:00:00Z</updated><published>2023-03-22T00:00:00Z</published><summary type="html">We released Quarkus 2.16.5.Final, the fifth maintenance release of our 2.16 release train. As usual, it contains bugfixes and documentation improvements. It should be a safe upgrade for anyone already using 2.16. If you are not already using 2.16, please refer to our migration guide. Full changelog You can get...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-03-22T00:00:00Z</dc:date></entry><entry><title>How to investigate 7 common problems in production</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/21/how-investigate-7-common-problems-production" /><author><name>Michael Dawson</name></author><id>4c05d237-a97b-4969-a2c7-7cc828ba1918</id><updated>2023-03-21T07:00:00Z</updated><published>2023-03-21T07:00:00Z</published><summary type="html">&lt;p&gt;Leveraging the experience shared in the Node.js reference architecture can help you minimize problems in production. However, it's a fact of life that problems will still occur and you need to do problem determination. This installment of the ongoing &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;&lt;u&gt;Node.js Reference Architecture&lt;/u&gt;&lt;/a&gt; series covers the Node.js reference architecture team’s experience with respect to how you can investigate common problems when they do occur.&lt;/p&gt; &lt;p&gt;7 Common problems include:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Memory leaks&lt;/li&gt; &lt;li&gt;Hangs or slow performance&lt;/li&gt; &lt;li&gt;Application failures&lt;/li&gt; &lt;li&gt;Unhandled promise rejections or exceptions&lt;/li&gt; &lt;li&gt;Resource leaks&lt;/li&gt; &lt;li&gt;Network issues&lt;/li&gt; &lt;li&gt;Natives crashes&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Read the series so far:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Part 1: &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview"&gt;&lt;u&gt;Overview of the Node.js reference architecture&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 2: &lt;a href="https://developer.ibm.com/blogs/nodejs-reference-architectire-pino-for-logging/"&gt;&lt;u&gt;Logging in Node.js&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 3: &lt;a href="https://developers.redhat.com/articles/2021/05/17/introduction-nodejs-reference-architecture-part-3-code-consistency"&gt;&lt;u&gt;Code consistency in Node.js&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 4: &lt;a href="https://developers.redhat.com/articles/2021/06/22/introduction-nodejs-reference-architecture-part-4-graphql-nodejs"&gt;&lt;u&gt;GraphQL in Node.js&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 5: &lt;a href="https://developers.redhat.com/articles/2021/08/26/introduction-nodejs-reference-architecture-part-5-building-good-containers"&gt;&lt;u&gt;Building good containers&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 6: &lt;a href="https://developers.redhat.com/articles/2021/12/03/introduction-nodejs-reference-architecture-part-6-choosing-web-frameworks"&gt;&lt;u&gt;Choosing web frameworks&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 7: &lt;a href="https://developers.redhat.com/articles/2022/03/02/introduction-nodejs-reference-architecture-part-7-code-coverage"&gt;&lt;u&gt;Code Coverage&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 8: &lt;a href="https://developers.redhat.com/articles/2022/04/11/introduction-nodejs-reference-architecture-part-8-typescript"&gt;&lt;u&gt;Typescript&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 9: &lt;a href="https://developers.redhat.com/articles/2022/08/09/8-elements-securing-nodejs-applications"&gt;&lt;u&gt;Securing Node.js applications&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 10: &lt;a href="https://developers.redhat.com/articles/2022/11/03/nodejs-reference-architecture-part-10-accessibility"&gt;&lt;u&gt;Accessibility&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 11: &lt;a href="https://developers.redhat.com/articles/2022/12/21/typical-development-workflows"&gt;&lt;u&gt;Typical development workflows&lt;/u&gt;&lt;/a&gt;&lt;/li&gt; &lt;li&gt;Part 12: &lt;a href="https://developers.redhat.com/articles/2023/02/22/installing-nodejs-modules-using-npm-registry"&gt;Npm development&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Part 13: Problem determination&lt;/strong&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Preparing for production problems&lt;/h2&gt; &lt;p&gt;Investigating problems in production most often requires tools and processes to be in place and approved in advance. It is important to define the tools you’ll rely on and to get approval for those tools to either already be in place or to be installed when needed. Production environments are often tightly controlled, and doing this in advance will speed up your ability to get information when problems do occur.&lt;/p&gt; &lt;p&gt;The team typically used one of the following to capture a set of metrics regularly in production:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Existing application performance management (APM) offering&lt;/li&gt; &lt;li&gt;Custom solution leveraging available platform tools (for example, those provided by a Cloud provider or Red Hat OpenShift)&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;You can read more about the suggested approach for capturing metrics in the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/metrics.md"&gt;metrics&lt;/a&gt; section. When those metrics have identified there is a problem, it’s time to kick off the problem determination process and try to match the symptoms to one of the common problems. &lt;/p&gt; &lt;h2&gt;APM or custom solution&lt;/h2&gt; &lt;p&gt;From the discussion we had within the team, one of the key factors of whether you want to use an APM or custom solution is whether you are operating a service or developing an application that will be operated by your customers.&lt;/p&gt; &lt;p&gt;If you are operating a service where you own all deployments an existing application performance management (APM) solution can make sense if budget is not an issue. They offer the advantage of requiring less up front investment and leveraging a solution designed to help investigate problems. The team has had success with using &lt;a href="https://www.dynatrace.com/"&gt;Dynatrace&lt;/a&gt;, &lt;a href="https://www.ibm.com/cloud/instana"&gt;Instana&lt;/a&gt;, and &lt;a href="https://newrelic.com/"&gt;&lt;u&gt;NewRelic&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you develop applications that will be operated by customers, adding a dependency on a specific APM for problem determination is not recommended. The cost may be an issue for some customers, while others may already have standardized on a specific APM.&lt;/p&gt; &lt;p&gt;Whichever you choose, you’ll want to be ready to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Instrument the application in order to capture:  &lt;ul&gt;&lt;li&gt;&lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/logging.md"&gt;Logs&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/metrics.md"&gt;Metrics&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/distributed-tracing.md"&gt;Traces&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt;Generate and extract heap snapshots.&lt;/li&gt; &lt;li&gt;Generate and extract core dumps.&lt;/li&gt; &lt;li&gt;Dynamically change log levels.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/problem-determination.md#implementation"&gt;implementation&lt;/a&gt; section of the problem-determination section of the Node.js reference architecture has some good suggestions on how to do these based on our experience.&lt;/p&gt; &lt;h2&gt;Investigating specific problems&lt;/h2&gt; &lt;p&gt;Once you have your chosen approach in place (APM or custom) and your metrics start reporting an issue, the general flow the team discussed and agreed on for problem determination was to:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Match the symptoms reported by traces, metrics, logs, and health checks to one of the common problems.&lt;/li&gt; &lt;li&gt;Follow a set of steps from easiest to hardest to confirm/refute the suspected problem.&lt;/li&gt; &lt;li&gt;When confirmed, capture additional information. &lt;/li&gt; &lt;li&gt;Repeat as necessary until you’ve narrowed it down to the right problem.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;I won’t repeat the content from the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/problem-determination.md#common-problems"&gt;&lt;u&gt;common problems&lt;/u&gt;&lt;/a&gt; section of the problem determination section in the reference architecture. But each problem includes:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Symptoms - how to identify the problem from the traces, metrics, logs, or health checks.&lt;/li&gt; &lt;li&gt;Approach - the steps and tools used by the team to investigate and capture more information for that problem.&lt;/li&gt; &lt;li&gt;Guidance - guidance and things to look out for when applying the approach suggested.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The  &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture/blob/main/docs/operations/problem-determination.md#common-problems"&gt;&lt;u&gt;common problems&lt;/u&gt;&lt;/a&gt; section covers the following problems based on the team's experience:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Memory leaks&lt;/li&gt; &lt;li&gt;Hangs or slow performance&lt;/li&gt; &lt;li&gt;Application failures&lt;/li&gt; &lt;li&gt;Unhandled promise rejections or exceptions&lt;/li&gt; &lt;li&gt;Resource leaks&lt;/li&gt; &lt;li&gt;Network issues&lt;/li&gt; &lt;li&gt;Natives crash&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;The information in that section for each of those problems should help you identify what problem your application is encountering based on the symptoms and then investigate to get more information as to what might be causing it to occur.&lt;/p&gt; &lt;h2&gt;Coming next&lt;/h2&gt; &lt;p&gt;I hope that this quick overview of the problem determination section of the Node.js Reference Architecture, along with the team discussions that led to that content, has been helpful and that the information shared in the architecture helps you in your future problem determination efforts.&lt;/p&gt; &lt;p&gt;We plan to cover new topics regularly as part of the &lt;a href="https://developers.redhat.com/blog/2021/03/08/introduction-to-the-node-js-reference-architecture-part-1-overview/"&gt;&lt;u&gt;Node.js reference architecture series&lt;/u&gt;&lt;/a&gt;. Until the next installment, we invite you to visit the &lt;a href="https://github.com/nodeshift/nodejs-reference-architecture"&gt;&lt;u&gt;Node.js reference architecture repository&lt;/u&gt;&lt;/a&gt; on GitHub, where you will see the work we have done and future topics. &lt;/p&gt; &lt;p&gt;To learn more about what Red Hat is up to on the Node.js front, check out our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;&lt;u&gt;Node.js page&lt;/u&gt;&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/21/how-investigate-7-common-problems-production" title="How to investigate 7 common problems in production"&gt;How to investigate 7 common problems in production&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2023-03-21T07:00:00Z</dc:date></entry><entry><title type="html">Kogito 1.35.0 released!</title><link rel="alternate" href="https://blog.kie.org/2023/03/kogito-1-35-0-released.html" /><author><name>Cristiano Nicolai</name></author><id>https://blog.kie.org/2023/03/kogito-1-35-0-released.html</id><updated>2023-03-17T12:58:52Z</updated><content type="html">We are glad to announce that the Kogito 1.35.0 release is now available! This goes hand in hand with , release. From a feature point of view, we have included a series of new features and bug fixes, including: * Flyway integration for JDBC based persistence * We have removed the use of kogito.persistence.auto.ddl property to initialize tables across persistence in Kogito. When using kogito-addons-quarkus-persistence-jdbc extension, we must set the quarkus.datasource.db-kind property to locate appropriate schema scripts. * GET &lt;processId&gt; API does not return all active process instances any longer, but a subset of them. The number of instances returned is determined by the property kogito.process.instances.limit, whose default value is 1000. If you need finer control of those instances, please check data-index functionality.  * Users that want to reduce memory footprint might replace quarkus postgresql extension  (quakus-jdbc-postgresql) by extension (quarkus-embedded-postgresql) * Data Index allows to be configured to use http-connector to consume Knative eventing. BREAKING CHANGES * Removed kogito.persistence.auto.ddl property for JDBC persistence addons ( kogito-addons-quarkus-persistence-jdbc and kogito-addons-springboot-persistence-jdbc). Please refer to using Flyway integration in order to handle database creation. For more details head to the complete . All artifacts are available now: * Kogito runtime artifacts are available on Maven Central. * Kogito examples can be found . * Kogito images are available on . * Kogito operator is available in the in OpenShift and Kubernetes. * Kogito tooling 0.27.0 artifacts are available at the . A detailed changelog for 1.35.0 can be found in . New to Kogito? Check out our website . Click the "Get Started" button. The post appeared first on .</content><dc:creator>Cristiano Nicolai</dc:creator></entry><entry><title type="html">How to live reload applications on WildFly</title><link rel="alternate" href="http://www.mastertheboss.com/jbossas/jboss-deploy/how-to-live-reload-applications-on-wildfly/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/jbossas/jboss-deploy/how-to-live-reload-applications-on-wildfly/</id><updated>2023-03-17T07:38:45Z</updated><content type="html">In this article we will discuss how to use WildFly Maven plugin to automate the redeployment of applications on WildFly when you apply some changes to the application class files. What is WildFly Maven plugin WildFly Maven Plugin is a plugin for Apache Maven that provides a set of goals to manage and deploy applications ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>How to easily deploy OpenShift on Azure via GitOps, Part 2</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/17/deploy-openshift-azure-gitops-part-2" /><author><name>Ignacio Lago, Pablo Castelo</name></author><id>b4b5b5b2-d750-44b7-b5d9-6b64a946271b</id><updated>2023-03-17T07:00:00Z</updated><published>2023-03-17T07:00:00Z</published><summary type="html">&lt;p&gt;Deploying OpenShift in Azure utilizing OpenShift GitOps makes it not only easier but enables you to achieve faster time to market, better collaboration, more efficient workflows, and ensures access to the cloud's scalability, flexibility, and reliability.&lt;/p&gt; &lt;p&gt;In this article, we are going to follow the same process from our &lt;a href="https://developers.redhat.com/articles/2023/03/16/how-deploy-openshift-azure-gui-part-1"&gt;previous article&lt;/a&gt; but use a GitOps approach for this.&lt;/p&gt; &lt;h2&gt;Step 1: Setting up the credentials for Azure in GitOps&lt;/h2&gt; &lt;p&gt;First of all, we have to fork the &lt;a href="https://github.com/ignaciolago/acm-ocp-azure.git"&gt;repository&lt;/a&gt;, so we can make changes to it since we are going to use the integration from the previous article. In between Red Hat Advanced Cluster Management for Kubernetes and ArgoCD to deploy the changes (ArgoCD reads a Git Repository). Now we can take a look at the repo:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;tree -L 2 . ├── README.md ├── bootstrap │ ├── argocd │ └── deploy └── resources ├── 01-acm-operator ├── 02-azure-credentials └── 03-azure-cluster 7 directories, 1 file&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We need to edit the folders for the &lt;strong&gt;02-azure-credentials&lt;/strong&gt; and &lt;strong&gt;03-azure-cluster.&lt;/strong&gt;&lt;/p&gt; &lt;p&gt;Let us take a look at what is inside the folders. The./resources/02-azure-credentials are shown in the following snippet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;tree -L 2./resources/02-azure-credentials ./resources/02-azure-credentials ├── 00-azure-credentials-namespace copy.yaml ├── 01-azure-credentials-secret.yaml └── kustomization.yaml 0 directories, 3 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here we need to edit the file 01-azure-credentials-secret.yaml. Let us take a look at that file and what we have to replace. In this case, the baseDomainResourceGroupName¹, PullSecret², the baseDomain³, the SSH Private&lt;strong&gt;⁵⁴&lt;/strong&gt; and Public&lt;strong&gt;⁵&lt;/strong&gt; Keys:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat resources/02-azure-credentials/01-azure-credentials-secret.yaml --- apiVersion: v1 kind: Secret type: Opaque metadata: annotations: argocd.argoproj.io/sync-wave: "1" name: azure-credentials namespace: azure-credentials labels: cluster.open-cluster-management.io/type: azr cluster.open-cluster-management.io/credentials: "" stringData: # 1 resource group created for the DNS to be used as base in the installation baseDomainResourceGroupName: openenv-jrh4l cloudName: AzurePublicCloud # 2 ClientId + ClientSecret + TenantId + SubscriptionId in json format osServicePrincipal.json: '{"clientId":"client_id_here","clientSecret":"client_secret_here","tenantId":"tenant_id_here","subscriptionId":"subscription_id_here"}' # 3 base DNS to be used as base in the installation baseDomain: jrh4l.azure.something.io # Your pull secret for pulling the images pullSecret: &gt; {pull_secret_here} # Your Private Key for accessing the machines 4 ssh-privatekey: | -----BEGIN OPENSSH PRIVATE KEY----- private_key_here -----END OPENSSH PRIVATE KEY----- # Your Public Key for accessing the machines 5 ssh-publickey: &gt; ssh-rsa public_key_here httpProxy: "" httpsProxy: "" noProxy: "" additionalTrustBundle: ""&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 2: Install the cluster in Azure via GitOps&lt;/h2&gt; &lt;p&gt;Let us take a look at what is inside the folders. The./resources/02-azure-credentials are shown in the following snippet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;tree -L 2./resources/03-azure-cluster ./resources/03-azure-cluster ├── 00-azure-cluster-namespace.yaml ├── 01-azure-managedclusterset copy.yaml ├── 02-azure-cluster.yaml └── kustomization.yaml 0 directories, 4 files&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Here we need to edit the file 02-azure-cluster.yaml. Let us take a look at that file and what we have to replace. In this case, the Cluster Name, Cluster Namespace, the baseDomain, the baseDomainResourceGroupName, Base64 encoding of the pull secret, install-config.yaml in base64 encoding, ssh-private key, osServicePrincipal.json:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat resources/03-azure-cluster/02-azure-cluster.yaml --- apiVersion: v1 kind: Namespace metadata: name: cluster01 --- apiVersion: hive.openshift.io/v1 kind: ClusterDeployment metadata: # 1 Cluster name in this case is cluster01 name: 'cluster01' # 2 Cluster namespace in this case is cluster01 namespace: 'cluster01' labels: cloud: 'Azure' region: 'westeurope' vendor: OpenShift cluster.open-cluster-management.io/clusterset: 'azure-clusters' spec: # 3 base DNS to be used as base in the installation baseDomain: jrh4l.azure.something.io clusterName: 'cluster01' controlPlaneConfig: servingCertificates: {} installAttemptsLimit: 1 installed: false platform: azure: # 4 resource group created for the DNS to be used in the installation baseDomainResourceGroupName: openenv-jrh4l credentialsSecretRef: name: cluster01-azure-creds region: westeurope cloudName: AzurePublicCloud provisioning: installConfigSecretRef: name: cluster01-install-config sshPrivateKeySecretRef: name: cluster01-ssh-private-key imageSetRef: #quay.io/openshift-release-dev/ocp-release:4.10.53-x86_64 name: img4.10.53-x86-64-appsub pullSecretRef: name: cluster01-pull-secret --- apiVersion: cluster.open-cluster-management.io/v1 kind: ManagedCluster metadata: labels: cloud: Azure region: westeurope name: 'cluster01' vendor: OpenShift cluster.open-cluster-management.io/clusterset: 'azure-clusters' name: 'cluster01' spec: hubAcceptsClient: true --- apiVersion: hive.openshift.io/v1 kind: MachinePool metadata: name: cluster01-worker namespace: 'cluster01' spec: clusterDeploymentRef: name: 'cluster01' name: worker platform: azure: osDisk: diskSizeGB: 128 type: Standard_D2s_v3 zones: - "1" - "2" - "3" replicas: 3 --- apiVersion: v1 kind: Secret metadata: name: cluster01-pull-secret namespace: 'cluster01' data: # 5 Base64 encoding of the pull secret .dockerconfigjson: &gt;- pull_secret_in_base64 type: kubernetes.io/dockerconfigjson --- apiVersion: v1 kind: Secret metadata: name: cluster01-install-config namespace: 'cluster01' type: Opaque data: # 6 Base64 encoding of install-config yaml install-config.yaml: YXBpVmVyc2lvbjogdjENCm1ldGFkYXRhOg0KICBuYW1lOiAnY2x1c3RlcjAxJw0KIyBiYXNlIEROUyB0byBiZSB1c2VkIGFzIGJhc2UgaW4gdGhlIGluc3RhbGxhdGlvbg0KYmFzZURvbWFpbjoganJoNGwuYXp1cmUuc29tZXRoaW5nLmlvDQpjb250cm9sUGxhbmU6DQogIGFyY2hpdGVjdHVyZTogYW1kNjQNCiAgaHlwZXJ0aHJlYWRpbmc6IEVuYWJsZWQNCiAgbmFtZTogbWFzdGVyDQogIHJlcGxpY2FzOiAzDQogIHBsYXRmb3JtOg0KICAgIGF6dXJlOg0KICAgICAgb3NEaXNrOg0KICAgICAgICBkaXNrU2l6ZUdCOiAxMjgNCiAgICAgIHR5cGU6ICBTdGFuZGFyZF9ENHNfdjMNCmNvbXB1dGU6DQotIGh5cGVydGhyZWFkaW5nOiBFbmFibGVkDQogIGFyY2hpdGVjdHVyZTogYW1kNjQNCiAgbmFtZTogJ3dvcmtlcicNCiAgcmVwbGljYXM6IDMNCiAgcGxhdGZvcm06DQogICAgYXp1cmU6DQogICAgICB0eXBlOiAgU3RhbmRhcmRfRDJzX3YzDQogICAgICBvc0Rpc2s6DQogICAgICAgIGRpc2tTaXplR0I6IDEyOA0KICAgICAgem9uZXM6DQogICAgICAtICIxIg0KICAgICAgLSAiMiINCiAgICAgIC0gIjMiDQpuZXR3b3JraW5nOg0KICBuZXR3b3JrVHlwZTogT1ZOS3ViZXJuZXRlcw0KICBjbHVzdGVyTmV0d29yazoNCiAgLSBjaWRyOiAxMC4xMjguMC4wLzE0DQogICAgaG9zdFByZWZpeDogMjMNCiAgbWFjaGluZU5ldHdvcms6DQogIC0gY2lkcjogMTAuMC4wLjAvMTYNCiAgc2VydmljZU5ldHdvcms6DQogIC0gMTcyLjMwLjAuMC8xNg0KcGxhdGZvcm06DQogIGF6dXJlOg0KICAgICMgcmVzb3VyY2UgZ3JvdXAgY3JlYXRlZCBmb3IgdGhlIEROUyB0byBiZSB1c2VkIGFzIGJhc2UgaW4gdGhlIGluc3RhbGxhdGlvbg0KICAgIGJhc2VEb21haW5SZXNvdXJjZUdyb3VwTmFtZTogb3BlbmVudi1qcmg0bA0KICAgIGNsb3VkTmFtZTogQXp1cmVQdWJsaWNDbG91ZA0KICAgIHJlZ2lvbjogd2VzdGV1cm9wZQ0KcHVsbFNlY3JldDogIiIgIyBza2lwLCBoaXZlIHdpbGwgaW5qZWN0IGJhc2VkIG9uIGl0J3Mgc2VjcmV0cw0Kc3NoS2V5OiB8LQ0KICAgIHNzaC1yc2ENCiAgICBwdWJsaWNfa2V5X2hlcmUNCg== --- apiVersion: v1 kind: Secret metadata: name: cluster01-ssh-private-key namespace: 'cluster01' stringData: # 7 we have to put our private ssh key in base64 here ssh-privatekey: &gt;- private_ssh_key_in_base64 type: Opaque --- apiVersion: v1 kind: Secret type: Opaque metadata: name: cluster01-azure-creds namespace: 'cluster01' stringData: # 8 we have to put the azure credentials in base 64 here osServicePrincipal.json: &gt;- azure_credentials_in_base64 --- apiVersion: agent.open-cluster-management.io/v1 kind: KlusterletAddonConfig metadata: name: 'cluster01' namespace: 'cluster01' spec: clusterName: 'cluster01' clusterNamespace: 'cluster01' clusterLabels: cloud: Azure vendor: OpenShift applicationManager: enabled: true policyController: enabled: true searchCollector: enabled: true certPolicyController: enabled: true iamPolicyController: enabled: true&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Before deploying it, we need to change the application sets so it points to our newly forked repository. Take a look at the deploy folder in the following snippets:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;tree -L 2 bootstrap/deploy bootstrap/deploy ├── 00-applicationset-acm │ ├── 00-acm-appproject.yaml │ ├── 01-acm-appset.yaml │ └── kustomization.yaml ├── 01-applicationset-azure-credentials │ ├── 00-azure-credentials-appproject.yaml │ ├── 01-azure-credentials-appset.yaml │ └── kustomization.yaml └── 02-applicationset-azure-cluster ├── 01-azure-cluster-appset.yaml └── kustomization.yaml 3 directories, 8 files&lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat bootstrap/deploy/01-applicationset-azure-credentials/01-azure-credentials-appset.yaml --- apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: azure-credentials namespace: openshift-gitops spec: generators: - clusters: {} template: metadata: name: azure-credentials-{{name}} spec: project: azure syncPolicy: automated: prune: true selfHeal: true source: # we have to replace this with our forked repository repoURL: https://github.com/ignaciolago/acm-ocp-azure.git targetRevision: main path: resources/02-azure-credentials destination: server: https://kubernetes.default.svc &lt;/code&gt;&lt;/pre&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cat bootstrap/deploy/02-applicationset-azure-cluster/01-azure-cluster-appset.yaml --- apiVersion: argoproj.io/v1alpha1 kind: ApplicationSet metadata: name: azure-cluster namespace: openshift-gitops spec: generators: - clusters: {} template: metadata: name: azure-cluster-{{name}} spec: project: azure syncPolicy: automated: prune: true selfHeal: true source: # we have to replace this with our forked repository repoURL: https://github.com/ignaciolago/acm-ocp-azure.git targetRevision: main path: resources/03-azure-cluster destination: server: https://kubernetes.default.svc &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now we are ready to deploy. Run the following commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -k bootstrap/deploy/01-applicationset-azure-credentials oc apply -k bootstrap/deploy/02-applicationset-azure-cluster&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can track the process in the web GUI or get the logs from the pod in the namespace &lt;strong&gt;cluster01&lt;/strong&gt;.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc get pods -n cluster01 oc get logs pod-name -n cluster01&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Quick and easy deployment OpenShift on Azure&lt;/h2&gt; &lt;p&gt;Congrats on deploying OpenShift on Azure in only a few steps! In these two articles, we have shown two easy ways to deploy OpenShift on Azure: GUI and GitOps. Comment below if you have questions. As always, we welcome your feedback.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/17/deploy-openshift-azure-gitops-part-2" title="How to easily deploy OpenShift on Azure via GitOps, Part 2"&gt;How to easily deploy OpenShift on Azure via GitOps, Part 2&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ignacio Lago, Pablo Castelo</dc:creator><dc:date>2023-03-17T07:00:00Z</dc:date></entry><entry><title>How to easily deploy OpenShift on Azure using a GUI, Part 1</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/16/how-deploy-openshift-azure-gui-part-1" /><author><name>Ignacio Lago, Pablo Castelo</name></author><id>4d1ef3b6-2b6e-45da-8e1f-00d0c55938c3</id><updated>2023-03-16T07:00:00Z</updated><published>2023-03-16T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt; is a leading &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; application platform in building, deploying, and handling containerized applications at a large scale. The OpenShift tooling is built around the &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; approach for &lt;a href="http://developers.redhat.com/topics/ci-cd"&gt;continuous delivery&lt;/a&gt; using Git as a single source of truth for declarative infrastructure. All this can be achieved by using Red Hat OpenShift GitOps, Argo CD, and Red Hat Advanced Cluster Management for Kubernetes.&lt;/p&gt; &lt;p&gt;Deploying OpenShift on the Microsoft Azure Platform utilizing OpenShift GitOps makes it not only easier but enables you to achieve faster time to market, better collaboration, more efficient workflows, and ensures access to the cloud's scalability, flexibility, and reliability.&lt;/p&gt; &lt;p&gt;This is the first of two articles that demonstrates how to get your OpenShift cluster up and running in Azure quickly and easily without numerous failures in the process. This article demonstrates the steps to deploy OpenShift on Azure using the Red Hat Advanced Cluster Management. The following steps detail the prerequisites, setup, and deployment process utilizing a GUI.&lt;/p&gt; &lt;h2&gt;Step 1: Set up the prerequisites&lt;/h2&gt; &lt;p&gt;Follow these steps to configure Azure.&lt;/p&gt; &lt;p&gt;First, we have to be sure our quote is enough to deploy a Red Hat OpenShift Container Platform cluster.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Go to the &lt;a href="https://portal.azure.com"&gt;Azure Portal&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Subscription&lt;/strong&gt;. &lt;/li&gt; &lt;li&gt;Click &lt;strong&gt;Usage and Quotas &lt;/strong&gt;to filter using &lt;strong&gt;Region.&lt;/strong&gt;&lt;/li&gt; &lt;li&gt;Locate the &lt;strong&gt;Standard DSv3 Family vCPUs &lt;/strong&gt;and&lt;strong&gt; &lt;/strong&gt;check the quota.&lt;/li&gt; &lt;li&gt;We need at least &lt;strong&gt;24 Cores&lt;/strong&gt; for a small cluster.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Second, we need a resource group containing a DNS zone to deploy the cluster.&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Select &lt;strong&gt;Create Resource Group&lt;/strong&gt; and fill in the name and region. Then click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;To create the DNS zone in the newly created resource group, select Create &lt;strong&gt;DNS_Zone &lt;/strong&gt;and&lt;strong&gt; Create&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Then select&lt;strong&gt; Fill_Name&lt;/strong&gt; and &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Now we should have a &lt;strong&gt;Resource Group&lt;/strong&gt; with a DNS zone ready to use, as shown in Figure 1:&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_03.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_03.png?itok=ch92Q6wo" width="600" height="184" alt="The DNS zone in the newly created Resource Group." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: The DNS zone in the newly created Resource Group.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Step 2: Recover the Azure values&lt;/h2&gt; &lt;p&gt;We need to recover the credentials of the cloud provider, as shown in the following snippet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Your Azure environment details: Resource Group: openenv-5d9g3 DNS Zone: 5d9g3.azure.example.io Application: openenv-5d9g3 Application/Client/Service Principal ID: f4993d25-3cce-49f4-a68a-24bc1c166bd6 Password: 2Ci7Ba-boz.mK69c6m0wO5SLMtsuZpGUjy Tenant ID: example.onmicrosoft.com Subscription ID: 1d6c0f82-8e30-423a-9a1b-36fde35ab59c&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 3: Create additional resources (optional)&lt;/h2&gt; &lt;p&gt;We can also create resource groups for the virtual network/subnets and a network security group.&lt;/p&gt; &lt;p&gt;First, we create two resource groups for the networks and the cluster.&lt;/p&gt; &lt;p&gt;Create a resource group for the networks as follows:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Create &lt;strong&gt;Resource Group&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Fill in the &lt;strong&gt;Name and Region&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Inside the resource group, create a virtual network.  &lt;ul&gt;&lt;li aria-level="1"&gt;Fill in the name and select &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;In the virtual network, select subnets from the left menu.&lt;/li&gt; &lt;li aria-level="2"&gt;Then select &lt;strong&gt;new subnet&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Fill in the name (masters) and select &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;Repeat this process again for the workers' subnet.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Go back to the resource group to create a network security group. &lt;ul&gt;&lt;li aria-level="1"&gt;Fill in the name and select &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="2"&gt;In the network security group, select &lt;strong&gt;Inbound Security Rules&lt;/strong&gt; in the left menu.&lt;/li&gt; &lt;li aria-level="2"&gt;Add a new one for port 80.&lt;/li&gt; &lt;li aria-level="2"&gt;Repeat this for these ports 443/6443/22623.&lt;/li&gt; &lt;li aria-level="2"&gt;In the network security group, select subnets in the left menu, then associate the workers subnet.&lt;/li&gt; &lt;li aria-level="2"&gt;Repeat this for the masters subnet.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Create a resource group for the cluster as follows:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Create &lt;strong&gt;Resource Group&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Fill in the &lt;strong&gt;Name and Region.&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;Create&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Step 4: Install Red Hat Advanced Cluster Management via OpenShift GitOps&lt;/h2&gt; &lt;p&gt;Before installing the Red Hat Advanced Cluster Management operator, we need to deploy the ArgoCD first. We can do this by using this &lt;a href="https://github.com/ignaciolago/acm-ocp-azure"&gt;&lt;strong&gt;&lt;u&gt;repository&lt;/u&gt;&lt;/strong&gt;&lt;/a&gt;. Deploy the OpenShift GitOps ArgoCD by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -k bootstrap/argocd&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then install the Red Hat Advanced Cluster Management and integrate it with ArgoCD by running the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;oc apply -k bootstrap/deploy/00-applicationset-acm&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 5: Set up credentials for Azure via GUI&lt;/h2&gt; &lt;p&gt;Once we have those values, we need to create the credentials for the Azure network. We will set up the credential type, credential name, namespace, and cloud name with default values:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Credential Name = Azure&lt;/li&gt; &lt;li&gt;Namespace = Default&lt;/li&gt; &lt;li&gt;Base DNS domain = DNS Zone&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Click &lt;strong&gt;next&lt;/strong&gt; and then fill in the following values:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Client ID = Application/Client/Service Principal ID&lt;/li&gt; &lt;li aria-level="1"&gt;Client Secret = Password&lt;/li&gt; &lt;li aria-level="1"&gt;Subscription ID = Subscription ID&lt;/li&gt; &lt;li aria-level="1"&gt;Tenant ID = Tenant ID&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;We do not have to set any value for the proxy unless we have a customer proxy installation.&lt;/p&gt; &lt;p&gt;Now we need to &lt;a href="https://console.redhat.com/openshift/install/pull-secret"&gt;get the pull secret&lt;/a&gt; and keys. You can generate the keys using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ssh-keygen -t rsa&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Review the information for our credentials to use for the installation, as shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_04.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_04.png?itok=IzFijFBJ" width="600" height="498" alt="A screenshot the Azure credentials in the Review section." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: Review the Azure credentials.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Step 6: Installing OpenShift cluster via the GUI&lt;/h2&gt; &lt;p&gt;We need to recover the following field:&lt;/p&gt; &lt;p&gt;DNS Zone = Base DNS Domain&lt;/p&gt; &lt;p&gt;We will establish the OpenShift release version, a name for the cluster, and a cluster set. We can create a new cluster set for all the Azure clusters or use the default.&lt;/p&gt; &lt;p&gt;The following description corresponds to the numbers in the screenshot of the Red Hat Advanced Cluster Management GUI in Figure 3:&lt;/p&gt; &lt;p&gt;We can also change the cluster resources by adding extra values using the Yaml view (1), and edit the configuration using the&lt;strong&gt; &lt;/strong&gt;deploy&lt;strong&gt; &lt;/strong&gt;(2)&lt;strong&gt; &lt;/strong&gt;cluster-install (3) buttons. For example, you can add custom resource groups containing the base DNS (4) or a virtual network (5) with two subnets for masters (6) and workers (7), network security group, and the cluster (8).&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_05.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_05.jpg?itok=oirtGcwu" width="600" height="514" alt="A closer look at the Openshift Avance Cluster Manager GUI and how to add extra values using the Yaml view." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: A closer look at the Red Hat Advance Cluster Management GUI and how to add extra values using the Yaml view.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Next, set up the &lt;strong&gt;Nodes Size&lt;/strong&gt; for the cluster and disk type (see Figure 4):&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_06.jpg" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_06.jpg?itok=eGQPGBN9" width="600" height="513" alt="Change the Disk Type and other values in the Cluster Details page of OpenShift Cluster Manager GUI." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: Change the Disk Type and other values.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;For networking, we have to define if we are going to use SDN or OVN and if we want to use custom values, as shown in the following snippet:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Cluster network CIDR * : 10.128.0.0/14 Network host prefix * : 23 Service network CIDR * : 172.30.0.0/16 Machine CIDR * : 10.0.0.0/16&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We will use default values for the proxy and automation.&lt;/p&gt; &lt;p&gt;Finally, you can review and install the cluster information as shown in Figure 5.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/image_07.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/image_07.png?itok=TRib6kCa" width="600" height="272" alt="Click on create cluster to view the logs." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: Click on create cluster to see the logs.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Deploying OpenShift on Azure is easy&lt;/h2&gt; &lt;p&gt;In this article, we demonstrated how to install an OpenShift cluster on Azure using a GUI. In the &lt;a href="https://developers.redhat.com/articles/2023/03/17/deploy-openshift-azure-gitops-part-2"&gt;next article&lt;/a&gt;, you will learn a different method for installing OpenShift on Azure utilizing GitOps. If you have questions, feel free to leave a comment below. Your feedback is welcome.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/16/how-deploy-openshift-azure-gui-part-1" title="How to easily deploy OpenShift on Azure using a GUI, Part 1"&gt;How to easily deploy OpenShift on Azure using a GUI, Part 1&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Ignacio Lago, Pablo Castelo</dc:creator><dc:date>2023-03-16T07:00:00Z</dc:date></entry><entry><title>Quarkus 3.0.0.Alpha6 released</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-0-0-alpha6-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-0-0-alpha6-released/</id><updated>2023-03-15T00:00:00Z</updated><published>2023-03-15T00:00:00Z</published><summary type="html">A week after Alpha5, we are releasing Quarkus 3.0.0.Alpha6. While Alpha5 came with major changes such as the upgrade to Hibernate ORM 6, Alpha6 is a smaller release packed with bugfixes, enhancements, and improvements to our upgrade process. What’s new Among all the bugfixes and enhancements, two are worth mentioning...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-03-15T00:00:00Z</dc:date></entry><entry><title type="html">Dashbuilder Quarkus Extension 0.27.0</title><link rel="alternate" href="https://blog.kie.org/2023/03/dashbuilder-quarkus-extension-0-27-0.html" /><author><name>William Siqueira</name></author><id>https://blog.kie.org/2023/03/dashbuilder-quarkus-extension-0-27-0.html</id><updated>2023-03-14T20:55:27Z</updated><content type="html">We are glad to announce that Dashbuilder Quarkus Extension 0.27.0 is out with multiple improvements. In this article we are going to share the new features and you can see a little about it on our Quarkus insights session: NEW SAMPLES SCREEN Dashbuilder now provides a new samples screen that will allow users to start from a sample dashboard to build their own. The Samples screen allows users to try a dashboard and add to their project if they want to. It will be only available in development mode, but they can be part of the final jar by setting the property quarkus.dashbuilder.include-samples to true.  By default the link to the samples screen is displayed only if users do not have dashboards. When the user has dashboards then the samples UI can be access from DEV UI: PROPERTIES It is possible to set a dashboard property using application.properties. If you have a property in dashboard you can set the value for that property using quarkus.dashbuilder.properties.{dashboard name}.{property name}=value Users can also declare quarkus properties in the dashboard. For example, the property quarkus.http.cors  can be read directly from the YAML dashboard by declaring a property with the same name. KIE TOOLS 0.27.0 RELEASE This version brings Dashbuilder 0.27.0 with VSCode extension release and many other improvements which you can see on the announcement post: INSTALLATION To install the latest version just update the dependency version to 0.27.0     &lt;dependency&gt;       &lt;groupId&gt;io.quarkiverse.dashbuilder&lt;/groupId&gt;       &lt;artifactId&gt;quarkus-dashbuilder&lt;/artifactId&gt;       &lt;version&gt;999-SNAPSHOT&lt;/version&gt;     &lt;/dependency&gt; CONCLUSION Dashbuilder Quarkus Extension 0.27.0 is out and you can try it right now! Stay tuned for new features and let us know if you have any questions. The post appeared first on .</content><dc:creator>William Siqueira</dc:creator></entry><entry><title>A tutorial on Middleware Automation Collections</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/03/14/tutorial-middleware-automation-collections" /><author><name>Harsha Cherukuri</name></author><id>3bba2083-ef20-4dd8-911f-81088d04cc6d</id><updated>2023-03-14T07:00:00Z</updated><published>2023-03-14T07:00:00Z</published><summary type="html">&lt;p&gt;Getting up to speed with &lt;a href="https://github.com/ansible-middleware"&gt;Ansible Middleware&lt;/a&gt; Collections &lt;span&gt;is easy, and installing the &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt; only requires a few steps. This tutorial demonstrates &lt;/span&gt;six steps to configure a WildFly instance using Ansible by&lt;span&gt; preparing a local machine with the necessary tooling and then deploying an instance of WildFly using the WildFly collection provided by the &lt;/span&gt;Ansible Middleware.&lt;/p&gt; &lt;h2&gt;Step 1: Install Ansible Automation Platform&lt;/h2&gt; &lt;p&gt;First, let’s get Ansible &lt;span&gt;Automation Platform &lt;/span&gt;installed on the control node. A control node is a machine from which we will push the configurations to the managed nodes/hosts. Managed nodes are the ones we would like to configure and they can be defined under inventory. You can install Ansible &lt;span&gt;Automation Platform&lt;/span&gt; using your preferred method. Refer to the &lt;a href="https://docs.ansible.com/ansible/latest/installation_guide/intro_installation.html#installation-guide"&gt;installing Ansible&lt;/a&gt; documentation for details.&lt;/p&gt; &lt;h2&gt;Step 2: Install the galaxy server&lt;/h2&gt; &lt;p&gt;Ansible Content Collections is a distribution format for content that can include playbooks, roles, modules, and plugins. By default, while installing collections using ansible-galaxy collection, we are referring to the &lt;a href="https://galaxy.ansible.com"&gt;Galaxy server&lt;/a&gt;. But we can configure a different galaxy server like the Ansible automation hub by providing the details of the server on ansible.cfg. You can follow the &lt;a href="https://docs.ansible.com/ansible/latest/galaxy/user_guide.html#downloading-a-collection-from-automation-hub"&gt;guide&lt;/a&gt; to do so. In this tutorial, we would be using the &lt;a href="https://galaxy.ansible.com"&gt;Galaxy server&lt;/a&gt; to install and configure using the &lt;a href="https://galaxy.ansible.com/middleware_automation/wildfly"&gt;middleware_automation.wildfly&lt;/a&gt; collection.&lt;/p&gt; &lt;h2&gt;Step 3: Installing the ansible-navigator utility&lt;/h2&gt; &lt;p&gt;We will use an &lt;a href="https://docs.ansible.com/automation-controller/latest/html/userguide/execution_environments.html"&gt;execution environment&lt;/a&gt; and the &lt;a href="https://ansible-navigator.readthedocs.io/en/latest/installation/#linux"&gt;ansible-navigator&lt;/a&gt; utility to perform the automation and provisioning of the WildFly instance. Install and configure ansible-navigator based on the documentation for your target operating system. To perform the execution of the automation, the &lt;a href="https://quay.io/repository/ansible-middleware/ansible-middleware-ee"&gt;ansible-middleware-ee&lt;/a&gt; execution environment provided by the team includes all of the Ansible Content Collections &lt;span&gt;and their dependencies. So there is no need to install any additional components on the control node. We would be using our execution environment as it already includes all of our latest collections so we don’t have to set it up again.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Once ansible-navigator has been installed, execute the following command to browse all of the collections that are included within the execution environment image:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-navigator --eei quay.io/ansible-middleware/ansible-middleware-ee:latest collections&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the following output, we see the list of all the collections available in our execution environment.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt; Name Version Shadowed Type Path 0│ansible.builtin 2.12.5.post0 False contained /usr/local/lib/python3.8/site-packages/ansible 1│ansible.netcommon 3.0.0 False contained /usr/share/ansible/collections/ansible_collections/ansible 2│ansible.posix 1.4.0 False contained /usr/share/ansible/collections/ansible_collections/ansible 3│ansible.utils 2.6.1 False contained /usr/share/ansible/collections/ansible_collections/ansible 4│community.general 5.0.0 False contained /usr/share/ansible/collections/ansible_collections/communi 5│middleware_automation.amq 0.0.2 False contained /usr/share/ansible/collections/ansible_collections/middlew 6│middleware_automation.infinispan 1.0.3 False contained /usr/share/ansible/collections/ansible_collections/middlew 7│middleware_automation.jcliff 0.0.21 False contained /usr/share/ansible/collections/ansible_collections/middlew 8│middleware_automation.jws 0.0.3 False contained /usr/share/ansible/collections/ansible_collections/middlew 9│middleware_automation.keycloak 1.0.4 False contained /usr/share/ansible/collections/ansible_collections/middlew 10│middleware_automation.redhat_csp_download 1.2.2 False contained /usr/share/ansible/collections/ansible_collections/middlew 11│middleware_automation.wildfly 1.0.2 False contained /usr/share/ansible/collections/ansible_collections/middlew &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 4: Setting up the inventory&lt;/h2&gt; &lt;p&gt;Let's now set up a WildFly instance. Create an inventory file that includes a Red Hat Enterprise Linux 8 instance, the IP address of the instance, and login information for ansible to access it. We are using SSH keys instead of passwords. So these SSH keys are created on the controller node and we provide the path of the private key in the inventory file. Our inventory file looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;[wildfly] wildfly-0 ansible_host=10.0.148.43 ansible_user=root ansible_ssh_private_key_file=”path to your private key” &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 5: Installing and configuring WildFly&lt;/h2&gt; &lt;p&gt;Here is our Ansible Playbook &lt;code&gt;wildfly.yml&lt;/code&gt; for installing and configuring WildFly:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-yaml"&gt;--- - name: "Installation and configuration" hosts: wildfly vars: wildfly_install_workdir: '/opt' install_name: "wildfly" wildfly_user: "{{ install_name }}" wildfly_config_base: standalone-ha.xml wildfly_version: "26.0.0.Final" wildfly_home: "{{ wildfly_install_workdir }}/{{ install_name }}-{{ wildfly_version }}" collections: - middleware_automation.wildfly tasks: - name: Include wildfly role ansible.builtin.include_role: name: middleware_automation.wildfly.wildfly_install - name: "Set up for Wildfly instance" include_role: name: middleware_automation.wildfly.wildfly_systemd vars: wildfly_config_base: 'standalone-ha.xml' wildfly_basedir_prefix: "/opt/{{ install_name }}" wildfly_config_name: "{{ install_name }}" wildfly_instance_name: "{{ install_name }}" service_systemd_env_file: "/etc/{{ install_name }}.conf" service_systemd_conf_file: "/usr/lib/systemd/system/{{ install_name }}.service" &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 6: Run the Ansible Playbook&lt;/h2&gt; &lt;p&gt;Now, run the Ansible Playbook using ansible-navigator and the execution environment to configure WildFly on the remote node as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ansible-navigator --eei quay.io/ansible-middleware/ansible-middleware-ee:latest run wildfly.yml -i inventory -m stdout --become&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the playbook has completed executing, ssh into the instance and check the status and health check of the deployed WildFly service. We can do so with the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ssh root@10.0.148.43 curl http://127.0.0.1:9990/health &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The following code snippet shows the output:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;[{"name" : "boot-errors", "outcome" : true},{"name" : "deployments-status", "outcome" : true},{"name" : "server-state", "outcome" : true, "data" : [{ "value" : "running" }]},{"name" : "live-server", "outcome" : true},{"name" : "started-server", "outcome" : true},{ "outcome" : true }]&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check the status of the WildFly service using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ssh root@10.0.148.43 systemctl status wildfly &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In the following output, we can see the WildFly service is running without any errors:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;● wildfly.service - JBoss EAP (standalone mode) Loaded: loaded (/usr/lib/systemd/system/wildfly.service; enabled; vendor preset: disabled) Active: active (running) since Thu 2023-03-02 18:02:44 EST; 4 days ago Main PID: 52667 (standalone.sh) Tasks: 45 (limit: 23417) Memory: 263.6M CGroup: /system.slice/wildfly.service ├─52667 /bin/sh /opt/wildfly-26.0.0.Final/bin/standalone.sh -c wildfly.xml -b 0.0.0.0 -Djboss.server.config.dir=/opt/wildfly&gt; └─52741 java -D[Standalone] -server -Xms64m -Xmx512m -XX:MetaspaceSize=96M -XX:MaxMetaspaceSize=256m -Djava.net.preferIPv4St&gt; Mar 02 18:02:47 wildfly-0 standalone.sh[52741]: 18:02:47,802 INFO [org.jboss.modcluster] (ServerService Thread Pool -- 84) MODCLUSTER0&gt; Mar 02 18:02:47 wildfly-0 standalone.sh[52741]: 18:02:47,829 INFO [org.wildfly.extension.undertow] (MSC service thread 1-1) WFLYUT0006&gt; Mar 02 18:02:47 wildfly-0 standalone.sh[52741]: 18:02:47,864 INFO [org.jboss.as.connector.subsystems.datasources] (MSC service thread &gt; Mar 02 18:02:47 wildfly-0 standalone.sh[52741]: 18:02:47,953 INFO [org.jboss.as.patching] (MSC service thread 1-2) WFLYPAT0050: WildFl&gt; Mar 02 18:02:47 wildfly-0 standalone.sh[52741]: 18:02:47,988 INFO [org.jboss.as.server.deployment.scanner] (MSC service thread 1-1) WF&gt; Mar 02 18:02:48 wildfly-0 standalone.sh[52741]: 18:02:48,002 INFO [org.jboss.ws.common.management] (MSC service thread 1-3) JBWS022052&gt; Mar 02 18:02:48 wildfly-0 standalone.sh[52741]: 18:02:48,179 INFO [org.jboss.as.server] (Controller Boot Thread) WFLYSRV0212: Resuming&gt; Mar 02 18:02:48 wildfly-0 standalone.sh[52741]: 18:02:48,182 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0025: WildFly Full 26&gt; Mar 02 18:02:48 wildfly-0 standalone.sh[52741]: 18:02:48,183 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0060: Http management&gt; Mar 02 18:02:48 wildfly-0 standalone.sh[52741]: 18:02:48,183 INFO [org.jboss.as] (Controller Boot Thread) WFLYSRV0051: Admin console l&gt; lines 1-20/20 (END)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We can also validate the instance using our &lt;a href="https://github.com/ansible-middleware/wildfly-cluster-demo/blob/main/validate.yml"&gt;validate.yml&lt;/a&gt;. This will check if the instance is running and if the WildFly port is accessible for use.&lt;/p&gt; &lt;h2&gt;Explore other tutorials and resources&lt;/h2&gt; &lt;p&gt;In this tutorial, we demonstrated how to set up and provision an instance of WildFly using the Ansible Content Collections &lt;span&gt;for WildFly. We can also deploy JBoss EAP instead of the open-source WildFly using the same collection. For more information on deploying JBoss EAP refer to &lt;/span&gt;&lt;a href="https://developers.redhat.com/articles/2022/02/08/automate-and-deploy-jboss-eap-cluster-ansible#"&gt;Automate and deploy a JBoss EAP cluster with Ansible&lt;/a&gt;&lt;span&gt;. To deploy WildFly or JBoss EAP on multiple instances, you can refer to our &lt;/span&gt;&lt;a href="https://github.com/ansible-middleware/wildfly-cluster-demo"&gt;wildfly-cluster-demo&lt;/a&gt;&lt;span&gt;. &lt;/span&gt;&lt;/p&gt; &lt;p&gt;&lt;span&gt;For a more complex scenario, check out our &lt;/span&gt;&lt;a href="https://github.com/ansible-middleware/flange-demo"&gt;Flange project demo&lt;/a&gt;&lt;span&gt;, which uses the JBoss EAP, &lt;/span&gt;&lt;a href="https://access.redhat.com/products/red-hat-single-sign-on"&gt;Red Hat Single Sign-On&lt;/a&gt;&lt;span&gt;, and &lt;/span&gt;&lt;a href="https://developers.redhat.com/products/datagrid/overview"&gt;Red Hat Data Grid&lt;/a&gt;&lt;span&gt; as a cache and &lt;/span&gt;&lt;a href="https://access.redhat.com/collections/red-hat-jboss-core-services-collection"&gt;Red Hat Middleware Core Services Collection&lt;/a&gt;&lt;span&gt; as a load balancer. Also, our &lt;/span&gt;&lt;a href="https://console.redhat.com/ansible/automation-hub/repo/published/redhat/jboss_eap/"&gt;redhat.jboss_eap&lt;/a&gt;&lt;span&gt; collection is available on Ansible automation hub. Note that this collection is a Beta release and for &lt;/span&gt;&lt;a href="https://access.redhat.com/support/offerings/techpreview"&gt;Technical Preview&lt;/a&gt;&lt;span&gt;.&lt;/span&gt;&lt;/p&gt; &lt;p&gt;You can check out the other collections and demos within the GitHub organization: &lt;a href="https://github.com/ansible-middleware"&gt;ansible-middleware&lt;/a&gt; and the &lt;a href="https://ansiblemiddleware.com"&gt;Middleware Automation Collections&lt;/a&gt; &lt;span&gt;website.&lt;/span&gt;&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/03/14/tutorial-middleware-automation-collections" title="A tutorial on Middleware Automation Collections"&gt;A tutorial on Middleware Automation Collections&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Harsha Cherukuri</dc:creator><dc:date>2023-03-14T07:00:00Z</dc:date></entry></feed>
